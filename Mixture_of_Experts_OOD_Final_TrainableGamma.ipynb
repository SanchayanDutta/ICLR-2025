{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanchayanDutta/ICLR-2025/blob/main/Mixture_of_Experts_OOD_Final_TrainableGamma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TIiF8VkplNsr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def attention(P, Q, Z, activation=None):\n",
        "    B = Z.shape[0]\n",
        "    N = Z.shape[1] - 1\n",
        "    d = Z.shape[2] - 1\n",
        "\n",
        "    # Expand P and Q into (d+1) x (d+1)\n",
        "    P_full = torch.cat([P, torch.zeros(1, d).to(device)], dim=0)\n",
        "    P_full = torch.cat([P_full, torch.zeros(d+1, 1).to(device)], dim=1)\n",
        "    P_full[d, d] = 1.0  # extra dimension for the label\n",
        "\n",
        "    Q_full = torch.cat([Q, torch.zeros(1, d).to(device)], dim=0)\n",
        "    Q_full = torch.cat([Q_full, torch.zeros(d+1, 1).to(device)], dim=1)\n",
        "\n",
        "    # A is eye(N+1) except the bottom-right corner is 0\n",
        "    A = torch.eye(N+1).to(device)\n",
        "    A[N, N] = 0.0\n",
        "\n",
        "    # Attn shape: [B, N+1, N+1]\n",
        "    Attn = torch.einsum('BNi, ij, BMj -> BNM', (Z, Q_full, Z))\n",
        "    if activation is not None:\n",
        "        Attn = activation(Attn)\n",
        "\n",
        "    # key shape: [B, N+1, d+1]\n",
        "    key = torch.einsum('ij, BNj -> BNi', (P_full, Z))\n",
        "\n",
        "    # Output shape: [B, N+1, d+1]\n",
        "    Output = torch.einsum('BNM, ML, BLi -> BNi', (Attn, A, key))\n",
        "    return Output / N\n",
        "\n",
        "class Transformer_F(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, N, d, var, run_mode, head_choice=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # allparam: [n_layer, n_head, 2, d, d]\n",
        "        #   - The \"2\" dimension is for (P, Q).\n",
        "        self.register_parameter(\n",
        "            'allparam',\n",
        "            nn.Parameter(torch.zeros(n_layer, n_head, 2, d, d))\n",
        "        )\n",
        "        # gamma: [n_layer, n_head, 1, N+1, d+1]\n",
        "        self.gamma = nn.Parameter(torch.ones(n_layer, n_head, 1, N+1, d+1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.allparam.normal_(0, var)  # initialize allparam\n",
        "\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.run_mode = run_mode\n",
        "        self.head_choice = head_choice  # used if run_mode=0\n",
        "\n",
        "        # Gating mechanism: one gating parameter per head.\n",
        "        # We let it be trainable only if run_mode==1\n",
        "        self.gate = nn.Parameter(\n",
        "            torch.zeros(n_head), requires_grad=(run_mode == 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, Z):\n",
        "        \"\"\"\n",
        "        Z has shape [B, N+1, d+1].\n",
        "          - The last entry in d+1 is typically the label dimension.\n",
        "        run_mode=0 => single-head usage, pick one head (head_choice).\n",
        "        run_mode=1 => multi-head usage with gating at test-time.\n",
        "        \"\"\"\n",
        "        B, N, d = Z.shape[0], Z.shape[1] - 1, Z.shape[2] - 1\n",
        "\n",
        "        # If run_mode=1, precompute the gating weights (for all heads)\n",
        "        if self.run_mode == 1:\n",
        "            # e.g. softmax if you want strictly positive + sum=1\n",
        "            # gate_weights_expanded = F.softmax(self.gate, dim=-1).view(-1, 1, 1, 1)\n",
        "            gate_weights_expanded = self.gate.view(-1, 1, 1, 1)\n",
        "\n",
        "        # --------------------------\n",
        "        # run_mode = 0 (single head)\n",
        "        # --------------------------\n",
        "        if self.run_mode == 0:\n",
        "            # If no head_choice is provided, default to 0\n",
        "            chosen_head = self.head_choice if self.head_choice is not None else 0\n",
        "\n",
        "            # Allocate R and T once, outside the layer loop\n",
        "            # R[i] and T[i] each: [B, N+1, d+1]\n",
        "            R = [torch.zeros_like(Z) for _ in range(self.n_layer)]\n",
        "            T = [torch.zeros_like(Z) for _ in range(self.n_layer)]\n",
        "\n",
        "            for i in range(self.n_layer):\n",
        "                # Save the old Z before the update\n",
        "                Zi = Z.clone()\n",
        "\n",
        "                Pij = self.allparam[i, chosen_head, 0, :, :]  # [d, d]\n",
        "                Qij = self.allparam[i, chosen_head, 1, :, :]  # [d, d]\n",
        "\n",
        "                R[i] = attention(Pij, Qij, Zi)  # => [B, N+1, d+1]\n",
        "\n",
        "                # Sum up older layers R[k], weighted by gamma[k]\n",
        "                T[i] = R[i] + sum(\n",
        "                    R[k] * self.gamma[k, chosen_head, :, :, :].expand(B, N+1, d+1)\n",
        "                    for k in range(i)\n",
        "                )\n",
        "\n",
        "                # Update Z\n",
        "                Z = Zi + T[i]\n",
        "\n",
        "            return Z\n",
        "\n",
        "        # --------------------------\n",
        "        # run_mode = 1 (multi-head)\n",
        "        # --------------------------\n",
        "        elif self.run_mode == 1:\n",
        "            # We'll store R[i][j], T[i][j] for each layer i, head j\n",
        "            # R[i][j] => [B, N+1, d+1]\n",
        "            R = [\n",
        "                [torch.zeros_like(Z) for _ in range(self.n_head)]\n",
        "                for _ in range(self.n_layer)\n",
        "            ]\n",
        "            T = [\n",
        "                [torch.zeros_like(Z) for _ in range(self.n_head)]\n",
        "                for _ in range(self.n_layer)\n",
        "            ]\n",
        "\n",
        "            for i in range(self.n_layer):\n",
        "                Zi = Z.clone()\n",
        "                head_attentions = []\n",
        "\n",
        "                for j in range(self.n_head):\n",
        "                    Pij = self.allparam[i, j, 0, :, :]  # [d, d]\n",
        "                    Qij = self.allparam[i, j, 1, :, :]  # [d, d]\n",
        "\n",
        "                    # Compute attention for layer i, head j\n",
        "                    R[i][j] = attention(Pij, Qij, Zi)\n",
        "\n",
        "                    # Sum up older layers R[k][j], weighted by gamma[k, j]\n",
        "                    T[i][j] = R[i][j] + sum(\n",
        "                        R[k][j] * self.gamma[k, j, :, :, :].expand(B, N+1, d+1)\n",
        "                        for k in range(i)\n",
        "                    )\n",
        "\n",
        "                    head_attentions.append(T[i][j])\n",
        "\n",
        "                # Stack heads into shape [n_head, B, N+1, d+1]\n",
        "                head_attentions = torch.stack(head_attentions, dim=0)\n",
        "\n",
        "                # Weighted sum across heads => [B, N+1, d+1]\n",
        "                attention_sum = (head_attentions * gate_weights_expanded).sum(dim=0)\n",
        "\n",
        "                # Update Z\n",
        "                Z = Zi + attention_sum\n",
        "\n",
        "            return Z\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid run_mode. Should be 0 or 1.\")\n",
        "\n",
        "    def zero_p(self):\n",
        "        # If you want to zero out P for each layer/head\n",
        "        with torch.no_grad():\n",
        "            for i in range(self.n_layer):\n",
        "                for j in range(self.n_head):\n",
        "                    self.allparam[i, j, 0, :, :].zero_()\n",
        "\n",
        "def in_context_loss(model, Z, y):\n",
        "    N = Z.shape[1] - 1\n",
        "    d = Z.shape[2] - 1\n",
        "    output = model(Z)  # shape [B, N+1, d+1]\n",
        "\n",
        "    # The \"test\" prediction often is output[:, N, d]\n",
        "    # y has shape [B] or [B, 1], so we do output[:,N,d] + y\n",
        "    diff = output[:, N, d] + y\n",
        "    loss = (diff ** 2).mean()\n",
        "    return loss\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Data-generation functions remain the same:\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "def generate_data(mode='normal',\n",
        "                  N=20,\n",
        "                  d=1,\n",
        "                  B=1000,\n",
        "                  shape_k=0.1,\n",
        "                  U=None,\n",
        "                  D=None,\n",
        "                  data_variance=1.0,\n",
        "                  data_mean=0.0):\n",
        "    \"\"\"\n",
        "    mode: 'normal', 'sphere', 'gamma', 'relu', or 'mlp'\n",
        "    N, d, B: as before\n",
        "    data_variance: variance of the Gaussian\n",
        "    data_mean: mean of the Gaussian\n",
        "    \"\"\"\n",
        "    std = math.sqrt(data_variance)\n",
        "    W = torch.FloatTensor(B, d).normal_(data_mean, std).to(device)\n",
        "    X = torch.FloatTensor(B, N, d).normal_(data_mean, std).to(device)\n",
        "    X_test = torch.FloatTensor(B, 1, d).normal_(data_mean, std).to(device)\n",
        "\n",
        "    if U is not None:\n",
        "        U = U.to(device)\n",
        "        D = D.to(device)\n",
        "        W = torch.mm(W, torch.inverse(D))\n",
        "        W = torch.mm(W, U.t())\n",
        "\n",
        "    if mode == 'sphere':\n",
        "        X.div_(X.norm(p=2, dim=2, keepdim=True))\n",
        "        X_test.div_(X_test.norm(p=2, dim=2, keepdim=True))\n",
        "    elif mode == 'gamma':\n",
        "        gamma_scales = np.random.gamma(shape=shape_k,\n",
        "                                       scale=(10 / shape_k)**0.5,\n",
        "                                       size=[B, N])\n",
        "        gamma_scales = torch.Tensor(gamma_scales).to(device).sqrt()\n",
        "        gamma_test_scales = np.random.gamma(shape=shape_k,\n",
        "                                            scale=(10 / shape_k)**0.5,\n",
        "                                            size=[B, 1])\n",
        "        gamma_test_scales = torch.Tensor(gamma_test_scales).to(device).sqrt()\n",
        "\n",
        "        X.div_(X.norm(p=2, dim=2, keepdim=True))\n",
        "        X_test.div_(X_test.norm(p=2, dim=2, keepdim=True))\n",
        "        X.mul_(gamma_scales.unsqueeze(-1))\n",
        "        X_test.mul_(gamma_test_scales.unsqueeze(-1))\n",
        "\n",
        "    elif mode == 'normal':\n",
        "        pass  # Already uses data_mean in normal_.\n",
        "    elif mode == 'relu':\n",
        "        return generate_data_relu(N=N, d=d, B=B, hidden_dim=d)\n",
        "    elif mode == 'mlp':\n",
        "        return generate_data_mlp(N=N, d=d, B=B, hidden_dim=d)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "    # If U and D are provided, apply them to X, X_test\n",
        "    if U is not None:\n",
        "        X = torch.einsum('ij, jk, BNk -> BNi', (U, D, X))\n",
        "        X_test = torch.einsum('ij, jk, BNk -> BNi', (U, D, X_test))\n",
        "\n",
        "    # Standard label generation\n",
        "    y = torch.einsum('bi,bni->bn', (W, X)).unsqueeze(2)  # shape [B, N, 1]\n",
        "    y_zero = torch.zeros(B, 1, 1).to(device)             # shape [B, 1, 1]\n",
        "    y_test = torch.einsum('bi,bni->bn', (W, X_test)).squeeze(1)  # shape [B]\n",
        "\n",
        "    # Combine data and label\n",
        "    X_comb = torch.cat([X, X_test], dim=1)  # [B, N+1, d]\n",
        "    y_comb = torch.cat([y, y_zero], dim=1)  # [B, N+1, 1]\n",
        "    Z = torch.cat([X_comb, y_comb], dim=2)  # [B, N+1, d+1]\n",
        "\n",
        "    return Z.to(device), y_test.to(device)\n",
        "\n",
        "def generate_data_inplace(Z, U=None, D=None):\n",
        "    B = Z.shape[0]\n",
        "    N = Z.shape[1] - 1\n",
        "    d = Z.shape[2] - 1\n",
        "\n",
        "    X = Z[:, :, 0:-1]\n",
        "    X.normal_(0, 1).to(device)\n",
        "    W = torch.FloatTensor(B, d).normal_(0, 1).to(device)\n",
        "\n",
        "    if U is not None:\n",
        "        U = U.to(device)\n",
        "        D = D.to(device)\n",
        "        W = torch.mm(W, torch.inverse(D))\n",
        "        W = torch.mm(W, U.t())\n",
        "        Z[:, :, 0:-1] = torch.einsum('ij, jk, BNk -> BNi', (U, D, X))\n",
        "\n",
        "    Z[:, :, -1] = torch.einsum('bi,bni->bn', (W, Z[:, :, 0:-1]))\n",
        "    y_test = Z[:, -1, -1].detach().clone()\n",
        "    Z[:, -1, -1].zero_()\n",
        "    return Z.to(device), y_test.to(device)\n",
        "\n",
        "def generate_data_sine(N=10, B=1000):\n",
        "    a = torch.FloatTensor(B).uniform_(0.1, 5).to(device)\n",
        "    p = torch.FloatTensor(B).uniform_(0, math.pi).to(device)\n",
        "\n",
        "    X = torch.FloatTensor(B, N).uniform_(-5, 5).to(device)\n",
        "    Y = a.unsqueeze(1) * torch.sin(p.unsqueeze(1) + X)\n",
        "\n",
        "    X = X.unsqueeze(-1)\n",
        "    Y = Y.unsqueeze(-1)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def generate_data_relu(mode='normal', N=20, d=1, B=1000, shape_k=0.1, U=None, D=None, hidden_dim=100):\n",
        "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
        "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(d, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, 1)\n",
        "    ).to(device)\n",
        "    model[0].weight.data.normal_(0, 0.1)\n",
        "    model[2].weight.data.normal_(0, 0.1)\n",
        "\n",
        "    # [B*N, 1] => reshape to [B, N, 1]\n",
        "    y = model(X.view(-1, d)).view(B, N, 1)\n",
        "    y_test = model(X_test.view(-1, d)).view(B, 1).squeeze(1)\n",
        "\n",
        "    y_zero = torch.zeros(B, 1, 1).to(device)\n",
        "    X_comb = torch.cat([X, X_test], dim=1)\n",
        "    y_comb = torch.cat([y, y_zero], dim=1)\n",
        "    Z = torch.cat([X_comb, y_comb], dim=2)\n",
        "    return Z, y_test\n",
        "\n",
        "def generate_data_mlp(N=20, d=1, B=1000, hidden_dim=100):\n",
        "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
        "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(d, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, d)\n",
        "    ).to(device)\n",
        "    model[0].weight.data.normal_(0, 1)\n",
        "    model[2].weight.data.normal_(0, 1)\n",
        "\n",
        "    X_MLP = model(X.view(-1, d)).view(B, N, d)\n",
        "    X_test_MLP = model(X_test.view(-1, d)).view(B, 1, d)\n",
        "\n",
        "    W = torch.FloatTensor(B, d).normal_(0, 1).to(device)\n",
        "    y = torch.einsum('bi,bni->bn', (W, X_MLP)).unsqueeze(2)\n",
        "    y_zero = torch.zeros(B, 1, 1).to(device)\n",
        "    y_test = torch.einsum('bi,bni->bn', (W, X_test_MLP)).squeeze(1)\n",
        "\n",
        "    X_comb = torch.cat([X_MLP, X_test_MLP], dim=1)\n",
        "    y_comb = torch.cat([y, y_zero], dim=1)\n",
        "    Z = torch.cat([X_comb, y_comb], dim=2)\n",
        "    return Z, y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/linear_transformer.py /content/"
      ],
      "metadata": {
        "id": "-RaQ44oDlpBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963acc4c-12da-48c0-f4f5-d4de42237fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: '/content/linear_transformer.py' and '/content/linear_transformer.py' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import linear_transformer"
      ],
      "metadata": {
        "id": "k4wTAj1MoD2I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "##############################################################################################################\n",
        "# Trains a linear Transformer with 1,2,3,4 layers\n",
        "# Plots the test loss of trained Transformer against 1,2,3,4 steps of gradient descent (with and without preconditioning)\n",
        "##############################################################################################################\n",
        "\n",
        "#use cuda if available, else use cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#torch.cuda.set_device(1)\n",
        "# import the model and some useful functions\n",
        "from linear_transformer import Transformer_F, attention, generate_data, in_context_loss, generate_data_inplace\n",
        "\n",
        "# set up some print options\n",
        "np.set_printoptions(precision = 2, suppress = True)\n",
        "torch.set_printoptions(precision=2)\n",
        "\n",
        "#begin logging\n",
        "cur_dir = 'log'\n",
        "os.makedirs(cur_dir, exist_ok=True)\n",
        "#f = open(cur_dir + '/rotation.log', \"a\", 1)\n",
        "#sys.stdout = f"
      ],
      "metadata": {
        "id": "c-Rnv5HirlCm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up problem parameters\n",
        "\n",
        "lr = 0.01\n",
        "clip_r = 0.01\n",
        "alg = 'adam'\n",
        "mode = 'normal'\n",
        "\n",
        "n_layer = 4  # number of layers of transformer\n",
        "N = 20     # context length\n",
        "d = 5        # dimension\n",
        "\n",
        "\n",
        "n_head = 3  # 1-headed attention\n",
        "B = 1000  # 1000 minibatch size\n",
        "var = 0.0001  # initializations scale of transformer parameter\n",
        "shape_k = 0.1  # shape_k: parameter for Gamma distributed covariates\n",
        "max_iters = 10  # Number of Iterations to run\n",
        "hist_stride = 1  # stride for saved model paramters in `train.ipynb'\n",
        "stride = 100\n",
        "\n",
        "# a convenience function for taking a step and clipping\n",
        "def clip_and_step(allparam, optimizer, clip_r = None):\n",
        "    norm_p=None\n",
        "    grad_all = allparam.grad\n",
        "    if clip_r is not None:\n",
        "        for l in range(grad_all.shape[0]):\n",
        "            for h in range(grad_all.shape[1]):\n",
        "                for t in range(grad_all.shape[2]):\n",
        "                    norm_p = grad_all[l,h,t,:,:].norm().item()\n",
        "                    if norm_p > clip_r:\n",
        "                        grad_all[l,h,t,:,:].mul_(clip_r/norm_p)\n",
        "    optimizer.step()\n",
        "    return norm_p\n",
        "\n",
        "#format for saving run data\n",
        "filename_format = '/variable_L_hist_{}_{}_{}.pth'\n",
        "n_layers = [1,2,3,4]  # number of layers of transformer\n",
        "seeds=[0,1,2,3,4]\n",
        "keys = []\n",
        "for s in seeds:\n",
        "    for n_layer in n_layers:\n",
        "        keys.append((s,n_layer,))"
      ],
      "metadata": {
        "id": "mE769w2br2vt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict05 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 3 heads:\n",
        "n_head = 3\n",
        "variances = [1, 2, 3]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict05[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 0.5)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 500\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict05[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "yv7ZUT_Mfidq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUd_3jWUftt0",
        "outputId": "c0ec1630-907e-4561-a40a-b239cda7bbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([-0.87, -1.48, -1.71, -2.14])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict1 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 1 head:\n",
        "n_head = 1\n",
        "variances = [1]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict1[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 1)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 1000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict1[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "pVx-P8FmIjQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYDAvlwYI1AW",
        "outputId": "9744c434-a1cc-4ede-b76f-d7dd9b8732da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([ 0.67, -0.43, -0.96, -1.19])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict15 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 3 heads:\n",
        "n_head = 3\n",
        "variances = [1, 2, 3]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict15[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 1.5)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 500\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict15[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "Uxk7Mmc9Jhl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2bk9n8OJxJN",
        "outputId": "84663b33-ee37-4505-ed2f-eb0a0d1f8f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([ 1.12, -0.17, -0.65, -0.74])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict2 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 1 head:\n",
        "n_head = 1\n",
        "variances = [2]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict2[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 2)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 1000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict2[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "s8g56_RVKljF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsA2rIq0KqcW",
        "outputId": "278bc6ee-b511-4aa4-ad39-9a1cde25b070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([ 1.82,  0.61,  0.16, -0.42])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict25 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 3 heads:\n",
        "n_head = 3\n",
        "variances = [1, 2, 3]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict25[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 2.5)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 500\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict25[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "WOuorzjRcmCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq2JA8VBcwjz",
        "outputId": "8efe0c72-a71a-486c-83d6-a864ad30d561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([2.14, 0.84, 0.36, 0.12])}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict3 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 1 head:\n",
        "n_head = 1\n",
        "variances = [3]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict3[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 3)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 1000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict3[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "Ty1ygUWTMQOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvnaE-HxMg0C",
        "outputId": "afbfe59c-f486-47bd-f3b3-aecaa263ea9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([2.52, 1.17, 0.75, 0.10])}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for the Transformer Model\n",
        "# - Trains allparam and gamma for each head in run_mode=0\n",
        "# - Combines them into a single run_mode=1 model\n",
        "# - Fine-tunes gating (and optionally gamma) at test-time\n",
        "########################################################\n",
        "\n",
        "loss_dict35 = {}\n",
        "store = 0\n",
        "Z_val = 0\n",
        "y_val = 0\n",
        "\n",
        "# Suppose we have 3 heads:\n",
        "n_head = 3\n",
        "variances = [1, 2, 3]  # Each head is trained on a different Gaussian variance\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict35[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters in two lists:\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh model for this head\n",
        "            head_model = Transformer_F(\n",
        "                n_layer, n_head, N, d, var, run_mode=0, head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for this head\n",
        "            data_variance = variances[head_idx]\n",
        "            gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)\n",
        "            U = torch.linalg.svd(gaus)[0].to(device)\n",
        "            D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U, D, data_variance=data_variance\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train allparam + gamma for this head\n",
        "            optimizer_head = torch.optim.Adam(\n",
        "                [head_model.allparam, head_model.gamma], lr=lr\n",
        "            )\n",
        "            epochs = 500\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store this head’s trained parameters\n",
        "            trained_state = head_model.state_dict()\n",
        "\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            # gamma:    [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into a single model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(n_layer, n_head, N, d, var, run_mode=1).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head’s part of allparam and gamma\n",
        "        # from the individually trained heads\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on test data\n",
        "        #         (Optionally also gamma)\n",
        "        # ----------------------------------------------\n",
        "        np.random.seed(99)\n",
        "        torch.manual_seed(99)\n",
        "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D, 3.5)\n",
        "        Z_test = Z_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # Typically we freeze allparam & gamma, train only gating\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "\n",
        "        # If you want to fine-tune gamma as well:\n",
        "        # model.gamma.requires_grad = True  # <--- optional\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 500\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute final test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict35[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, loss_dict05 will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "IpmJY654NFPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict35"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cni8HzTbNLFE",
        "outputId": "77a0a097-7a04-4a1d-c72c-182cbf0b7439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([2.81, 1.52, 1.04, 0.79])}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# plot final test loss against N\n",
        "####################################\n",
        "\n",
        "fig_dir = 'figures'\n",
        "os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "# Increase the width of the figure (e.g., 12 inches wide, 8 inches tall)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "# Plot for variance = 0.5\n",
        "losses05 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict05.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses05[idx, :] = loss_dict05[key]\n",
        "losses_mean05 = torch.mean(losses05, axis=0)\n",
        "losses_std05 = torch.std(losses05, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean05,\n",
        "    color='red',\n",
        "    lw=3,\n",
        "    label=r'$\\sigma^2 = 0.5$'\n",
        ")\n",
        "\n",
        "# Plot for variance = 1 (make dotted)\n",
        "losses1 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict1.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses1[idx, :] = loss_dict1[key]\n",
        "losses_mean1 = torch.mean(losses1, axis=0)\n",
        "losses_std1 = torch.std(losses1, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean1,\n",
        "    color='blue',\n",
        "    lw=3,\n",
        "    linestyle=':',  # dotted line\n",
        "    label=r'$\\sigma^2 = 1$'\n",
        ")\n",
        "\n",
        "# Plot for variance = 1.5\n",
        "losses15 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict15.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses15[idx, :] = loss_dict15[key]\n",
        "losses_mean15 = torch.mean(losses15, axis=0)\n",
        "losses_std15 = torch.std(losses15, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean15,\n",
        "    color='green',\n",
        "    lw=3,\n",
        "    label=r'$\\sigma^2 = 1.5$'\n",
        ")\n",
        "\n",
        "# Plot for variance = 2 (make dotted)\n",
        "losses2 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict2.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses2[idx, :] = loss_dict2[key]\n",
        "losses_mean2 = torch.mean(losses2, axis=0)\n",
        "losses_std2 = torch.std(losses2, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean2,\n",
        "    color='pink',\n",
        "    lw=3,\n",
        "    linestyle=':',  # dotted line\n",
        "    label=r'$\\sigma^2 = 2$'\n",
        ")\n",
        "\n",
        "# Plot for variance = 2.5\n",
        "losses25 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict25.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses25[idx, :] = loss_dict25[key]\n",
        "losses_mean25 = torch.mean(losses25, axis=0)\n",
        "losses_std25 = torch.std(losses25, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean25,\n",
        "    color='violet',\n",
        "    lw=3,\n",
        "    label=r'$\\sigma^2 = 2.5$'\n",
        ")\n",
        "\n",
        "# Plot for variance = 3 (make dotted)\n",
        "losses3 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict3.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses3[idx, :] = loss_dict3[key]\n",
        "losses_mean3 = torch.mean(losses3, axis=0)\n",
        "losses_std3 = torch.std(losses3, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean3,\n",
        "    color='grey',\n",
        "    lw=3,\n",
        "    linestyle=':',  # dotted line\n",
        "    label=r'$\\sigma^2 = 3$'\n",
        ")\n",
        "\n",
        "# Plot for variance = 3.5\n",
        "losses35 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict35.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses35[idx, :] = loss_dict35[key]\n",
        "losses_mean35 = torch.mean(losses35, axis=0)\n",
        "losses_std35 = torch.std(losses35, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean35,\n",
        "    color='black',\n",
        "    lw=3,\n",
        "    label=r'$\\sigma^2 = 3.5$'\n",
        ")\n",
        "\n",
        "plt.ylabel('log(Loss)', fontsize=30)\n",
        "plt.xlabel('Number of Layers/Steps', fontsize=30)\n",
        "ax.tick_params(axis='both', which='major', labelsize=30, width=3, length=10)\n",
        "ax.tick_params(axis='both', which='minor', labelsize=20, width=3, length=5)\n",
        "\n",
        "# Adjust legend properties to make it smaller and place it outside\n",
        "ax.legend(\n",
        "    fontsize=20,            # smaller font size for the legend\n",
        "    loc='upper left',\n",
        "    bbox_to_anchor=(1.05, 1.0),\n",
        "    borderaxespad=0.,\n",
        "    labelspacing=0.4,       # adjust space between legend entries\n",
        "    handlelength=3          # length of the legend lines\n",
        ")\n",
        "\n",
        "# Use 'rect' in tight_layout to leave space on the right for the legend\n",
        "plt.tight_layout(rect=[0, 0, 0.8, 1])  # 0.8 leaves some room on the right\n",
        "\n",
        "plt.savefig(fig_dir + '/variable-L-plot.pdf', dpi=600)"
      ],
      "metadata": {
        "id": "Ob2R88z7KrE_",
        "outputId": "ce44199a-19e1-4b06-88d5-1b79e501ac08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-6e36e8c2f879>:17: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std05 = torch.std(losses05, axis=0) / 10\n",
            "<ipython-input-26-6e36e8c2f879>:33: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std1 = torch.std(losses1, axis=0) / 10\n",
            "<ipython-input-26-6e36e8c2f879>:50: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std15 = torch.std(losses15, axis=0) / 10\n",
            "<ipython-input-26-6e36e8c2f879>:66: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std2 = torch.std(losses2, axis=0) / 10\n",
            "<ipython-input-26-6e36e8c2f879>:83: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std25 = torch.std(losses25, axis=0) / 10\n",
            "<ipython-input-26-6e36e8c2f879>:99: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std3 = torch.std(losses3, axis=0) / 10\n",
            "<ipython-input-26-6e36e8c2f879>:116: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  losses_std35 = torch.std(losses35, axis=0) / 10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for a 3-Component GMM\n",
        "# Each head is trained on a single component (non-zero mean),\n",
        "# but the overall mixture has mean 0.\n",
        "########################################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppose these are the means, variances, and weights for 3 components\n",
        "# whose *weighted* mean is zero:\n",
        "means = [2.5, 0.0, -1.0]      # e.g. 0.2*2.5 + 0.3*0.0 + 0.5*(-1.0) = 0\n",
        "variances = [0.5, 1.0, 2.0]   # per-component variance\n",
        "weights = [0.2, 0.3, 0.5]     # must sum to 1\n",
        "\n",
        "# We'll have 3 heads, one per component\n",
        "n_head = 3\n",
        "\n",
        "# Example hyperparameters\n",
        "lr = 1e-3\n",
        "B = 1000            # total batch size\n",
        "epochs = 300\n",
        "shape_k = 0.1\n",
        "mode = 'normal'\n",
        "N = 20\n",
        "d = 1\n",
        "var = 0.01          # variance for Transformer_F parameter init\n",
        "n_layers = [1, 2, 3, 4]   # example\n",
        "\n",
        "# Prepare logging\n",
        "loss_dict_gmm = {}\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict_gmm[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters for allparam and gamma.\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh single-head model (run_mode=0, head_choice=head_idx)\n",
        "            head_model = Transformer_F(\n",
        "                n_layer=n_layer,\n",
        "                n_head=n_head,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                var=var,\n",
        "                run_mode=0,      # single-head usage\n",
        "                head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for THIS head => single Gaussian component\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U=None, D=None,\n",
        "                data_variance=variances[head_idx],\n",
        "                data_mean=means[head_idx]\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train this head's parameters => both allparam + gamma\n",
        "            optimizer_head = torch.optim.Adam([\n",
        "                head_model.allparam,\n",
        "                head_model.gamma      # <--- also train gamma\n",
        "            ], lr=lr)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store the trained parameters for this head\n",
        "            trained_state = head_model.state_dict()\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            # gamma: [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into one model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            N=N,\n",
        "            d=d,\n",
        "            var=var,\n",
        "            run_mode=1\n",
        "        ).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head's portion of allparam + gamma with the trained values\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on GMM test data\n",
        "        # We'll sample B points from the entire mixture (all 3 components).\n",
        "        # The mixture is constructed so that the overall mean is zero.\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        # \"comp_indices\" = which component each sample belongs to\n",
        "        comp_indices = np.random.choice(range(n_head), size=B, p=weights)\n",
        "\n",
        "        # We'll accumulate the mixture data in lists, then cat them\n",
        "        Z_list = []\n",
        "        y_list = []\n",
        "        for i in range(n_head):\n",
        "            # How many from component i?\n",
        "            mask = (comp_indices == i)\n",
        "            num_i = mask.sum()\n",
        "            if num_i == 0:\n",
        "                continue\n",
        "\n",
        "            # Generate data_mean=means[i], data_variance=variances[i],\n",
        "            # but only for \"num_i\" samples\n",
        "            Z_temp, y_temp = generate_data(\n",
        "                mode=mode,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                B=num_i,               # <--- generate exactly num_i points\n",
        "                shape_k=shape_k,\n",
        "                U=None,\n",
        "                D=None,\n",
        "                data_variance=variances[i],\n",
        "                data_mean=means[i]\n",
        "            )\n",
        "            Z_list.append(Z_temp)\n",
        "            y_list.append(y_temp)\n",
        "\n",
        "        # Combine test data from all components => full GMM sample\n",
        "        Z_test = torch.cat(Z_list, dim=0).to(device)\n",
        "        y_test = torch.cat(y_list, dim=0).to(device)\n",
        "\n",
        "        # Fine-tune gating only\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "        model.gamma.requires_grad = True  # <--- freeze gamma if you want\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate, model.gamma], lr=lr)\n",
        "        fine_tune_iters = 5000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict_gmm[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, \"loss_dict_gmm\" will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "KjrSsF_nPNUs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict_gmm"
      ],
      "metadata": {
        "id": "3ySd5T6QPYjg",
        "outputId": "49f2ee12-33b7-4754-ea6a-a3d0893f3e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([ 0.41, -0.46, -0.99, -1.54])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for a 3-Component GMM\n",
        "# Each head is trained on a single component (non-zero mean),\n",
        "# but the overall mixture has mean 0.\n",
        "########################################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppose these are the means, variances, and weights for 3 components\n",
        "# whose *weighted* mean is zero:\n",
        "means = [2.5, 0.0, -1.0]      # e.g. 0.2*2.5 + 0.3*0.0 + 0.5*(-1.0) = 0\n",
        "variances = [0.5, 1.0, 2.0]   # per-component variance\n",
        "weights = [0.2, 0.3, 0.5]     # must sum to 1\n",
        "\n",
        "# We'll have 3 heads, one per component\n",
        "n_head = 3\n",
        "\n",
        "# Example hyperparameters\n",
        "lr = 1e-3\n",
        "B = 1000            # total batch size\n",
        "epochs = 300\n",
        "shape_k = 0.1\n",
        "mode = 'normal'\n",
        "N = 20\n",
        "d = 1\n",
        "var = 0.01          # variance for Transformer_F parameter init\n",
        "n_layers = [1, 2, 3, 4]   # example\n",
        "\n",
        "# Prepare logging\n",
        "loss_dict_gmm = {}\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict_gmm[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters for allparam and gamma.\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh single-head model (run_mode=0, head_choice=head_idx)\n",
        "            head_model = Transformer_F(\n",
        "                n_layer=n_layer,\n",
        "                n_head=n_head,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                var=var,\n",
        "                run_mode=0,      # single-head usage\n",
        "                head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for THIS head => single Gaussian component\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U=None, D=None,\n",
        "                data_variance=variances[head_idx],\n",
        "                data_mean=means[head_idx]\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train this head's parameters => both allparam + gamma\n",
        "            optimizer_head = torch.optim.Adam([\n",
        "                head_model.allparam,\n",
        "                head_model.gamma      # <--- also train gamma\n",
        "            ], lr=lr)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store the trained parameters for this head\n",
        "            trained_state = head_model.state_dict()\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            # gamma: [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into one model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            N=N,\n",
        "            d=d,\n",
        "            var=var,\n",
        "            run_mode=1\n",
        "        ).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head's portion of allparam + gamma with the trained values\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on GMM test data\n",
        "        # We'll sample B points from the entire mixture (all 3 components).\n",
        "        # The mixture is constructed so that the overall mean is zero.\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        # \"comp_indices\" = which component each sample belongs to\n",
        "        comp_indices = np.random.choice(range(n_head), size=B, p=weights)\n",
        "\n",
        "        # We'll accumulate the mixture data in lists, then cat them\n",
        "        Z_list = []\n",
        "        y_list = []\n",
        "        for i in range(n_head):\n",
        "            # How many from component i?\n",
        "            mask = (comp_indices == i)\n",
        "            num_i = mask.sum()\n",
        "            if num_i == 0:\n",
        "                continue\n",
        "\n",
        "            # Generate data_mean=means[i], data_variance=variances[i],\n",
        "            # but only for \"num_i\" samples\n",
        "            Z_temp, y_temp = generate_data(\n",
        "                mode=mode,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                B=num_i,               # <--- generate exactly num_i points\n",
        "                shape_k=shape_k,\n",
        "                U=None,\n",
        "                D=None,\n",
        "                data_variance=variances[i],\n",
        "                data_mean=means[i]\n",
        "            )\n",
        "            Z_list.append(Z_temp)\n",
        "            y_list.append(y_temp)\n",
        "\n",
        "        # Combine test data from all components => full GMM sample\n",
        "        Z_test = torch.cat(Z_list, dim=0).to(device)\n",
        "        y_test = torch.cat(y_list, dim=0).to(device)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Step 3: Fine-tune gating only\n",
        "        # BUT we want only the final layer of gamma to be learnable\n",
        "        # -----------------------------\n",
        "\n",
        "        # 1) Turn on grad for gate and gamma. Turn off for allparam.\n",
        "        model.gate.requires_grad_(True)\n",
        "        model.allparam.requires_grad_(False)\n",
        "        model.gamma.requires_grad_(True)   # We’ll zero out grads for all but the last layer.\n",
        "\n",
        "        # 2) Register a backward hook to zero out all gradients for layers [0 : n_layer-1].\n",
        "        def freeze_gamma_hook(grad):\n",
        "            \"\"\"\n",
        "            grad has shape [n_layer, n_head, 1, N+1, d+1].\n",
        "            We only keep gradient for grad[-1, ...]; set others to zero.\n",
        "            \"\"\"\n",
        "            mask = torch.zeros_like(grad)\n",
        "            mask[-1] = 1.0  # keep the last layer\n",
        "            return grad * mask\n",
        "\n",
        "        model.gamma.register_hook(freeze_gamma_hook)\n",
        "\n",
        "        # 3) Define the optimizer over model.gate and model.gamma.\n",
        "        optimizer2 = torch.optim.Adam([model.gate, model.gamma], lr=lr)\n",
        "\n",
        "        fine_tune_iters = 5000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict_gmm[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, \"loss_dict_gmm\" will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "ynM-Gh1VzzuR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict_gmm"
      ],
      "metadata": {
        "id": "6UqeesYo2YUk",
        "outputId": "b088313a-9326-412d-f8e4-b9788cdc3980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([0.69, 0.47, 0.39, 0.16])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for a 3-Component GMM\n",
        "# Each head is trained on a single component (non-zero mean),\n",
        "# but the overall mixture has mean 0.\n",
        "########################################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppose these are the means, variances, and weights for 3 components\n",
        "# whose *weighted* mean is zero:\n",
        "means = [2.5]      # E.g., 0.2*2.5 + 0.3*0.0 + 0.5*(-1.0) = 0\n",
        "variances = [0.5]   # per-component variance\n",
        "weights = [1]     # must sum to 1\n",
        "\n",
        "# We'll have 3 heads, one per component\n",
        "n_head = 1\n",
        "\n",
        "# Example hyperparameters\n",
        "lr = 1e-3\n",
        "B = 1000            # total batch size\n",
        "epochs = 300\n",
        "shape_k = 0.1\n",
        "mode = 'normal'\n",
        "N = 20\n",
        "d = 1\n",
        "var = 0.01          # variance for Transformer_F parameter init\n",
        "n_layers = [1, 2, 3, 4]   # example\n",
        "\n",
        "# Prepare logging\n",
        "loss_dict_gmm1 = {}\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict_gmm1[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters for allparam and gamma.\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh single-head model (run_mode=0, head_choice=head_idx)\n",
        "            head_model = Transformer_F(\n",
        "                n_layer=n_layer,\n",
        "                n_head=n_head,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                var=var,\n",
        "                run_mode=0,      # single-head usage\n",
        "                head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for THIS head => single Gaussian component\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U=None, D=None,\n",
        "                data_variance=variances[head_idx],\n",
        "                data_mean=means[head_idx]\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train this head's parameters => both allparam + gamma\n",
        "            optimizer_head = torch.optim.Adam([\n",
        "                head_model.allparam,\n",
        "                head_model.gamma      # <--- also train gamma\n",
        "            ], lr=lr)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store the trained parameters for this head\n",
        "            trained_state = head_model.state_dict()\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            # gamma: [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into one model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            N=N,\n",
        "            d=d,\n",
        "            var=var,\n",
        "            run_mode=1\n",
        "        ).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head's portion of allparam + gamma with the trained values\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on GMM test data\n",
        "        # We'll sample B points from the entire mixture (all 3 components).\n",
        "        # The mixture is constructed so that the overall mean is zero.\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        # \"comp_indices\" = which component each sample belongs to\n",
        "        comp_indices = np.random.choice(range(n_head), size=B, p=weights)\n",
        "\n",
        "        # We'll accumulate the mixture data in lists, then cat them\n",
        "        Z_list = []\n",
        "        y_list = []\n",
        "        for i in range(n_head):\n",
        "            # How many from component i?\n",
        "            mask = (comp_indices == i)\n",
        "            num_i = mask.sum()\n",
        "            if num_i == 0:\n",
        "                continue\n",
        "\n",
        "            # Generate data_mean=means[i], data_variance=variances[i],\n",
        "            # but only for \"num_i\" samples\n",
        "            Z_temp, y_temp = generate_data(\n",
        "                mode=mode,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                B=num_i,               # <--- generate exactly num_i points\n",
        "                shape_k=shape_k,\n",
        "                U=None,\n",
        "                D=None,\n",
        "                data_variance=variances[i],\n",
        "                data_mean=means[i]\n",
        "            )\n",
        "            Z_list.append(Z_temp)\n",
        "            y_list.append(y_temp)\n",
        "\n",
        "        # Combine test data from all components => full GMM sample\n",
        "        Z_test = torch.cat(Z_list, dim=0).to(device)\n",
        "        y_test = torch.cat(y_list, dim=0).to(device)\n",
        "\n",
        "        # Fine-tune gating only\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "        model.gamma.requires_grad = False  # <--- freeze gamma if you want\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 5000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict_gmm1[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, \"loss_dict_gmm\" will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "YU_CGrSIPfbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict_gmm1"
      ],
      "metadata": {
        "id": "MB0yXs4mP1eq",
        "outputId": "e8f5b6e8-4853-4348-bbf2-596f186a6f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([-0.52, -0.80, -1.03, -1.07])}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for a 3-Component GMM\n",
        "# Each head is trained on a single component (non-zero mean),\n",
        "# but the overall mixture has mean 0.\n",
        "########################################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppose these are the means, variances, and weights for 3 components\n",
        "# whose *weighted* mean is zero:\n",
        "means = [0]      # E.g., 0.2*2.5 + 0.3*0.0 + 0.5*(-1.0) = 0\n",
        "variances = [1]   # per-component variance\n",
        "weights = [1]     # must sum to 1\n",
        "\n",
        "# We'll have 3 heads, one per component\n",
        "n_head = 1\n",
        "\n",
        "# Example hyperparameters\n",
        "lr = 1e-3\n",
        "B = 1000            # total batch size\n",
        "epochs = 300\n",
        "shape_k = 0.1\n",
        "mode = 'normal'\n",
        "N = 20\n",
        "d = 1\n",
        "var = 0.01          # variance for Transformer_F parameter init\n",
        "n_layers = [1, 2, 3, 4]   # example\n",
        "\n",
        "# Prepare logging\n",
        "loss_dict_gmm2 = {}\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict_gmm2[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters for allparam and gamma.\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh single-head model (run_mode=0, head_choice=head_idx)\n",
        "            head_model = Transformer_F(\n",
        "                n_layer=n_layer,\n",
        "                n_head=n_head,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                var=var,\n",
        "                run_mode=0,      # single-head usage\n",
        "                head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for THIS head => single Gaussian component\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U=None, D=None,\n",
        "                data_variance=variances[head_idx],\n",
        "                data_mean=means[head_idx]\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train this head's parameters => both allparam + gamma\n",
        "            optimizer_head = torch.optim.Adam([\n",
        "                head_model.allparam,\n",
        "                head_model.gamma      # <--- also train gamma\n",
        "            ], lr=lr)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store the trained parameters for this head\n",
        "            trained_state = head_model.state_dict()\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            # gamma: [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into one model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            N=N,\n",
        "            d=d,\n",
        "            var=var,\n",
        "            run_mode=1\n",
        "        ).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head's portion of allparam + gamma with the trained values\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on GMM test data\n",
        "        # We'll sample B points from the entire mixture (all 3 components).\n",
        "        # The mixture is constructed so that the overall mean is zero.\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        # \"comp_indices\" = which component each sample belongs to\n",
        "        comp_indices = np.random.choice(range(n_head), size=B, p=weights)\n",
        "\n",
        "        # We'll accumulate the mixture data in lists, then cat them\n",
        "        Z_list = []\n",
        "        y_list = []\n",
        "        for i in range(n_head):\n",
        "            # How many from component i?\n",
        "            mask = (comp_indices == i)\n",
        "            num_i = mask.sum()\n",
        "            if num_i == 0:\n",
        "                continue\n",
        "\n",
        "            # Generate data_mean=means[i], data_variance=variances[i],\n",
        "            # but only for \"num_i\" samples\n",
        "            Z_temp, y_temp = generate_data(\n",
        "                mode=mode,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                B=num_i,               # <--- generate exactly num_i points\n",
        "                shape_k=shape_k,\n",
        "                U=None,\n",
        "                D=None,\n",
        "                data_variance=variances[i],\n",
        "                data_mean=means[i]\n",
        "            )\n",
        "            Z_list.append(Z_temp)\n",
        "            y_list.append(y_temp)\n",
        "\n",
        "        # Combine test data from all components => full GMM sample\n",
        "        Z_test = torch.cat(Z_list, dim=0).to(device)\n",
        "        y_test = torch.cat(y_list, dim=0).to(device)\n",
        "\n",
        "        # Fine-tune gating only\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "        model.gamma.requires_grad = False  # <--- freeze gamma if you want\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 5000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict_gmm2[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, \"loss_dict_gmm\" will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "TEuKC_lFQ6Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict_gmm2"
      ],
      "metadata": {
        "id": "hL_3h_CimtlK",
        "outputId": "67c43145-5c2a-4e44-b594-5711fa5f16f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0,): tensor([-2.25, -2.63, -2.79, -2.84])}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# Modified Training/Testing Code for a 3-Component GMM\n",
        "# Each head is trained on a single component (non-zero mean),\n",
        "# but the overall mixture has mean 0.\n",
        "########################################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppose these are the means, variances, and weights for 3 components\n",
        "# whose *weighted* mean is zero:\n",
        "means = [-1]      # E.g., 0.2*2.5 + 0.3*0.0 + 0.5*(-1.0) = 0\n",
        "variances = [2]   # per-component variance\n",
        "weights = [1]     # must sum to 1\n",
        "\n",
        "# We'll have 3 heads, one per component\n",
        "n_head = 1\n",
        "\n",
        "# Example hyperparameters\n",
        "lr = 1e-3\n",
        "B = 1000            # total batch size\n",
        "epochs = 300\n",
        "shape_k = 0.1\n",
        "mode = 'normal'\n",
        "N = 20\n",
        "d = 1\n",
        "var = 0.01          # variance for Transformer_F parameter init\n",
        "n_layers = [1, 2, 3, 4]   # example\n",
        "\n",
        "# Prepare logging\n",
        "loss_dict_gmm3 = {}\n",
        "seeds = [0]\n",
        "\n",
        "for sd in seeds:\n",
        "    key = (sd,)\n",
        "    loss_dict_gmm3[key] = torch.zeros(len(n_layers))\n",
        "\n",
        "    for idx, n_layer in enumerate(n_layers):\n",
        "        # ----------------------------------------------\n",
        "        # Step 1: Train each head separately (run_mode=0)\n",
        "        # ----------------------------------------------\n",
        "        # We'll store the per-head parameters for allparam and gamma.\n",
        "        head_params_allparam = []\n",
        "        head_params_gamma = []\n",
        "\n",
        "        for head_idx in range(n_head):\n",
        "            # Create a fresh single-head model (run_mode=0, head_choice=head_idx)\n",
        "            head_model = Transformer_F(\n",
        "                n_layer=n_layer,\n",
        "                n_head=n_head,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                var=var,\n",
        "                run_mode=0,      # single-head usage\n",
        "                head_choice=head_idx\n",
        "            ).to(device)\n",
        "            head_model.train()\n",
        "\n",
        "            # Generate training data for THIS head => single Gaussian component\n",
        "            Z_train, y_train = generate_data(\n",
        "                mode, N, d, B, shape_k, U=None, D=None,\n",
        "                data_variance=variances[head_idx],\n",
        "                data_mean=means[head_idx]\n",
        "            )\n",
        "            Z_train, y_train = Z_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Train this head's parameters => both allparam + gamma\n",
        "            optimizer_head = torch.optim.Adam([\n",
        "                head_model.allparam,\n",
        "                head_model.gamma      # <--- also train gamma\n",
        "            ], lr=lr)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                optimizer_head.zero_grad()\n",
        "                loss = in_context_loss(head_model, Z_train, y_train)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "\n",
        "            # Store the trained parameters for this head\n",
        "            trained_state = head_model.state_dict()\n",
        "            # allparam: [n_layer, n_head, 2, d, d]\n",
        "            head_params_allparam.append(trained_state['allparam'].clone().detach())\n",
        "            # gamma: [n_layer, n_head, 1, N+1, d+1]\n",
        "            head_params_gamma.append(trained_state['gamma'].clone().detach())\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 2: Combine heads into one model (run_mode=1)\n",
        "        # ----------------------------------------------\n",
        "        model = Transformer_F(\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            N=N,\n",
        "            d=d,\n",
        "            var=var,\n",
        "            run_mode=1\n",
        "        ).to(device)\n",
        "        combined_state = model.state_dict()\n",
        "\n",
        "        # Overwrite each head's portion of allparam + gamma with the trained values\n",
        "        for i_layer in range(n_layer):\n",
        "            for head_idx in range(n_head):\n",
        "                combined_state['allparam'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_allparam[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "                combined_state['gamma'][i_layer, head_idx, :, :, :] = \\\n",
        "                    head_params_gamma[head_idx][i_layer, head_idx, :, :, :]\n",
        "\n",
        "        model.load_state_dict(combined_state)\n",
        "        model.eval()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 3: Fine-tune gating parameters on GMM test data\n",
        "        # We'll sample B points from the entire mixture (all 3 components).\n",
        "        # The mixture is constructed so that the overall mean is zero.\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        # \"comp_indices\" = which component each sample belongs to\n",
        "        comp_indices = np.random.choice(range(n_head), size=B, p=weights)\n",
        "\n",
        "        # We'll accumulate the mixture data in lists, then cat them\n",
        "        Z_list = []\n",
        "        y_list = []\n",
        "        for i in range(n_head):\n",
        "            # How many from component i?\n",
        "            mask = (comp_indices == i)\n",
        "            num_i = mask.sum()\n",
        "            if num_i == 0:\n",
        "                continue\n",
        "\n",
        "            # Generate data_mean=means[i], data_variance=variances[i],\n",
        "            # but only for \"num_i\" samples\n",
        "            Z_temp, y_temp = generate_data(\n",
        "                mode=mode,\n",
        "                N=N,\n",
        "                d=d,\n",
        "                B=num_i,               # <--- generate exactly num_i points\n",
        "                shape_k=shape_k,\n",
        "                U=None,\n",
        "                D=None,\n",
        "                data_variance=variances[i],\n",
        "                data_mean=means[i]\n",
        "            )\n",
        "            Z_list.append(Z_temp)\n",
        "            y_list.append(y_temp)\n",
        "\n",
        "        # Combine test data from all components => full GMM sample\n",
        "        Z_test = torch.cat(Z_list, dim=0).to(device)\n",
        "        y_test = torch.cat(y_list, dim=0).to(device)\n",
        "\n",
        "        # Fine-tune gating only\n",
        "        model.gate.requires_grad = True\n",
        "        model.allparam.requires_grad = False\n",
        "        model.gamma.requires_grad = False  # <--- freeze gamma if you want\n",
        "\n",
        "        optimizer2 = torch.optim.Adam([model.gate], lr=lr)\n",
        "        fine_tune_iters = 5000\n",
        "        for t in range(fine_tune_iters):\n",
        "            optimizer2.zero_grad()\n",
        "            loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "        # ----------------------------------------------\n",
        "        # Step 4: Compute test loss and store\n",
        "        # ----------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            final_loss = in_context_loss(model, Z_test, y_test)\n",
        "            loss_dict_gmm3[key][idx] = final_loss.log().item()\n",
        "\n",
        "# After this loop completes, \"loss_dict_gmm\" will contain\n",
        "# the logged test losses for each seed and layer configuration."
      ],
      "metadata": {
        "id": "mCpPlSPLRDs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict_gmm3"
      ],
      "metadata": {
        "id": "pk1v7NoZnTJ5",
        "outputId": "a106c1f0-3817-4056-ab34-f705edd77eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loss_dict_gmm3' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9fee92ce667d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_dict_gmm3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_dict_gmm3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_dict_gmm"
      ],
      "metadata": {
        "outputId": "18f498c5-fb4a-496a-dcd5-30bf73a5e661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSwjfmZ06JcO"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.41, -0.46, -0.99, -1.54])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# plot final test loss against N\n",
        "####################################\n",
        "\n",
        "fig_dir = 'figures'\n",
        "os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "# Increase the width of the figure (e.g., 12 inches wide, 8 inches tall)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "# Plot for GMM\n",
        "losses_gmm = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict_gmm.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses_gmm[idx, :] = loss_dict_gmm[key]\n",
        "losses_mean_gmm = torch.mean(losses_gmm, axis=0)\n",
        "losses_std_gmm = torch.std(losses_gmm, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean_gmm,\n",
        "    color='black',\n",
        "    lw=3,\n",
        "    label='GMM'\n",
        ")\n",
        "\n",
        "# Plot for GMM 1\n",
        "losses_gmm1 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict_gmm1.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses_gmm1[idx, :] = loss_dict_gmm1[key]\n",
        "losses_mean_gmm1 = torch.mean(losses_gmm1, axis=0)\n",
        "losses_std_gmm1 = torch.std(losses_gmm1, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean_gmm1,\n",
        "    color='blue',\n",
        "    lw=3,\n",
        "    linestyle=':',  # dotted line\n",
        "    label='GMM 1'\n",
        ")\n",
        "\n",
        "# Plot for GMM 2\n",
        "losses_gmm2 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict_gmm2.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses_gmm2[idx, :] = loss_dict_gmm2[key]\n",
        "losses_mean_gmm2 = torch.mean(losses_gmm2, axis=0)\n",
        "losses_std_gmm2 = torch.std(losses_gmm2, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean_gmm2,\n",
        "    color='red',\n",
        "    lw=3,\n",
        "    linestyle=':',  # dotted line\n",
        "    label='GMM 2'\n",
        ")\n",
        "\n",
        "# Plot for GMM 3\n",
        "losses_gmm3 = torch.zeros(len(seeds), len(n_layers))\n",
        "keys = loss_dict_gmm3.keys()\n",
        "for idx, key in enumerate(keys):\n",
        "    losses_gmm3[idx, :] = loss_dict_gmm3[key]\n",
        "losses_mean_gmm3 = torch.mean(losses_gmm3, axis=0)\n",
        "losses_std_gmm3 = torch.std(losses_gmm3, axis=0) / 10\n",
        "\n",
        "ax.plot(\n",
        "    n_layers,\n",
        "    losses_mean_gmm3,\n",
        "    color='green',\n",
        "    lw=3,\n",
        "    linestyle=':',  # dotted line\n",
        "    label='GMM 3'\n",
        ")\n",
        "\n",
        "plt.ylabel('log(Loss)', fontsize=30)\n",
        "plt.xlabel('Number of Layers/Steps', fontsize=30)\n",
        "ax.tick_params(axis='both', which='major', labelsize=30, width=3, length=10)\n",
        "ax.tick_params(axis='both', which='minor', labelsize=20, width=3, length=5)\n",
        "\n",
        "# Adjust legend properties to make it smaller and place it outside\n",
        "ax.legend(\n",
        "    fontsize=20,            # smaller font size for the legend\n",
        "    loc='upper left',\n",
        "    bbox_to_anchor=(1.05, 1.0),\n",
        "    borderaxespad=0.,\n",
        "    labelspacing=0.4,       # adjust space between legend entries\n",
        "    handlelength=3          # length of the legend lines\n",
        ")\n",
        "\n",
        "# Use 'rect' in tight_layout to leave space on the right for the legend\n",
        "plt.tight_layout(rect=[0, 0, 0.8, 1])  # 0.8 leaves some room on the right\n",
        "\n",
        "plt.savefig(fig_dir + '/variable-L-plot.pdf', dpi=600)"
      ],
      "metadata": {
        "id": "U8gQH8i-o3GB",
        "outputId": "174b9ffc-da48-41ac-af91-171bd998db0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'keys'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-34fe17e6ace6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Plot for GMM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlosses_gmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict_gmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlosses_gmm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict_gmm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'keys'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}